---
title: "Causal Inference in the 2020 US Federal Election: While COVID-19 Lowered Trump's Support, Loss of Confidence Among Wealthier Voters Seems More Decisive"
title-block-banner: true
author: "Yiliu Cao"
thanks: "Code and data from this analysis are available at: https://github.com/yiliuc/covid_and_trump_loss.git"
date: "today"
date-format: "long"
abstract: "This study used the election data from MIT EDSL and COVID-19 data from JHU CSSE, to investigate the causal inference between COVID-19 and Donald Trump's loss during the 2020 US Federal Election. The main methodology used in this paper is Propensity Score Matching with testing different treatments. The primary finding is that the counties with high death per case rate seems to vote less for Trump with approximate treatment effect -0.01. Besides, it seems that the interaction effect with income seems to be more critical: the medium-high income levels counties seems to vote at maximum 3% less for Trump compared to those low income counties. Furthermore, this paper also conduct counterfactual analysis and indicate the Trump would pre-elect if there was no COVID. Furture analysis should focus more on the choice of covariates to predict propensity scores."
format: pdf
toc: true
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(here)
library(dplyr)
library(knitr)
library(ggplot2)
library(maps)
library(mapdata)
library(RColorBrewer)
library(gridExtra)
library(tools)
```

```{r}
#| echo: false
#| message: false

election_data <- read.csv(here("outputs/data/election_data_clean.csv"))
covid_data <- read.csv(here("outputs/data/covid_data_clean.csv"))
acs_data <- read.csv(here("outputs/data/acs_data_clean.csv"))
covid_election_data <- read.csv(here("outputs/data/covid_election.csv"))
data <- read.csv(here("outputs/data/merged_data.csv"))
```

# Introduction


# Data

The data used in this paper comprised five data sets from three different sources corresponding to different topics. The primary source is MIT Election Data Science Club which build open online data collections of the US Federal or Senate Election results, spanning from nation to county levels. The data extracted is called "County Presidential Election Returns 2000-2020" with about 70,000 rows containing the voting patterns for each candidate and party by county since 2000. Besides that, the data also indicate the types of voting, such as "EARLY VOTE" and "ELECTION DAY," for the same party in the a county. To analyze the voting patterns for Donald J. Trump, I only select the data for 2016 and 2020 US Federal Election for all counties and parties. Thus the filterd data set contains the number of votes through various ways for each county and party.

In addition to that, the study also uses the data regarding COVID-19 from the Center for Systems Science and Engineering at Johns Hopkins University




In this paper, the data used mainly consists of the voting patterns for the 2020 US Federal Election, COVID infection rate and socioeconomic variables in each county of the US. The voting data is taken from the MIT Election Data Science Lab, including voting for each party at the county level at each Federal Election from 2000 to 2020. The COVID data is from the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University. The socioeconomic data is taken from the American Community Survey.

## MIT Election Data Science Lab

The MIT Election Data Science Lab is a lab at MIT that collects and analyzes election data to support advances in election science. Their collected data is not limited to the presidential election results but also covers the midterm or US Senate results at the state or county level.

The data abstracted from MITEDSL in this paper is the "County Presidential Election Returns 2000-2020." It contains the number of votes for each party and the total votes at each US county from 2000 to the 2020 US Federal Election. In this paper, I will only use the data for the 2020 US Election and focus on the counties on the mainland of the US. The @tbl-election summarizes the essential variables.

```{r}
#| echo: false
#| message: false
#| label: tbl-election
#| tbl-cap: The summary of voting patterns in 2020 US Federal Presidential Election
# election_data %>% 
#   group_by(party, candidate) %>% 
#   summarise(total = sum(votes),
#             mean = round(mean(pct_vote), 2),
#             median = round(median(pct_vote), 2)) %>%
#   arrange(desc(total)) %>% 
#   rename(`Party` = party,
#          `Candidate` = candidate,
#          `Total Votes` = total,
#          `Mean Pct Vote` = mean,
#          `Median Pct Vote` = median) %>% 
#   kable()
```

The @tbl-election summarizes the voting patterns for each party during the 2020 U.S. Presidential Election. Undoubtedly, the two most popular parties are the Democrats and the Republicans. However, even though Republican has almost double the mean percent vote than Democrat, the total votes for Republican is less than for Democrat. This pattern may indicate that more people living in high-population states, such as California, voted for Biden. This finding aligns with the fact that Biden defeated Trump in 2020. I will explain more later.

## Center for Systems Science and Engineering at JHU

The Center for Systems Science and Engineering at JHU is at the Department of Civils and Engineering, which collects local, national, and global multidimensional data, including medicine, health care, disaster response, etc. During the pandemic, they collected the U.S. and international COVID cases and deaths and reported them on their GitHub. Their data is summarized by daily reports ranging from April 12, 2020, to March 9, 2023.

To illustrate the impact of COVID-19 on the 2020 Election to the greatest extent, the COVID data used in this paper will be the daily report on November 3, 2020, which is Election Day \[citation\]. This report contains the aggregated cases and deaths for each country and counties in the U.S.

## American Community Survey (ACS)

The American Community Survey is the survey conducted by the U.S. Census Bureau. The survey contains a variety of socio-economic variables for each county. Although there are different data tables from ACS, I will use the 2020 five-year estimate of DP02, DP03, and DP05.

These three tables cover the social, economic and demographic characteristics of each county in the U.S. By referring to my previous research, where I found the variables with the highest correlations to the COVID mortality rate, I will use the same variables in this paper. That said, I will extract the data regarding the educational attainment from DP02, especially the proportion of people having at least a bachelor's degree. Regarding the economy, I will take the proportion of people with private health insurance and the mean household income. Lastly, regarding the demographic factors, I will use the total population, the ratios of children and individuals above 85. I will also take the White and Black people percentage as well.

## Data Cleaning

```{r}
#| echo: false
#| message: false
#| label: tbl-covid
#| tbl-cap: The summary of COVID cases and deaths as of Election Day.
data %>% 
  group_by(winning_party) %>% 
  summarise(case = mean(cases),
            deaths = mean(deaths),
            inf = mean(infrate),
            dpc = mean(dpc),
            income = mean(mean_household_income)) %>% 
  rename(`Winning Party` = winning_party,
         `Cases` = case,
         `Deaths` = deaths,
         `Infection Rate` = inf,
         `Income` = income) %>% 
  kable()
```

Combining the three data sources, @tbl-covid shows the COVID cases and deaths and the infection and mortality rate for the county voting for the Democrat and Republican parties, respectively. Surprisingly, the average cases and fatalities of COVID-19 in the counties voting for the Democrats are significantly higher than that of the Republican party. However, the values of infection and mortality rate do not show the same dramatic difference. It also emphasizes that the counties (states) voting for the Democrat are population intensive, and counties with more residence generally have higher mean household income levels. We can observe this from the average income where the "Democratic" counties have about \$13k mean income more elevated than the "Republic" ones.

```{r}
#| echo: false
#| message: false
#| label: fig-boxplot
#| fig-cap: Summary of income levels for counties voting for Democratic and Republican

data  %>%
  ggplot(mapping = aes(x = winning_party, y = mean_household_income)) +
  geom_boxplot() +
  geom_jitter(alpha = 0.04, width = 0.3, height = 0) +
  theme_minimal() +
  labs(x = "Winning Party",
       y = "Mean Household Income")
```

@fig-boxplot compares the distribution of county income levels for the two parties. In general, counties voting for the Democrats have higher income levels than the Republicans. In addition, we can observe that the counties are intensively concentrated around income levels \$50k to \$75k, compared to the Democrats, where the counties are approximately uniformly distributed at each income level. Furthermore, barely any county has at least 150k voting for the Republican party. Both @tbl-covid and @fig-boxplot indicates that the Democrat is in favour of wealthy counties but poorer counties for Republican. Consequently, the more affluent counties (states) usually have more electoral votes than the poorer ones; this may provide insights into why Trump lost.

```{r}
#| echo: false
#| message: false

get_state_abbreviation <- function(state_names) {
  sapply(state_names, function(state_name) {
    state_data <- state.abb[match(state_name, state.name)]
    if (!is.na(state_data)) {
      return(state_data)
    } else {
      return(NA)  # changed from NULL to NA for consistency in a vector
    }
  })
}

county <- map_data("county")
state <- map_data("state")
county_clean <- county %>% 
  mutate(state = region,
         county = subregion,
         state = get_state_abbreviation(toTitleCase(state)),
         county = toTitleCase(county)) %>% 
  filter(region != "AK" & region != "HI") %>% 
  dplyr::select(-region, -subregion)

merged_data <- county_clean %>% 
  left_join(data, by = c("state", "county"))
```

```{r}
binned_data <- data %>%
  mutate(dpc_pctile = ntile(dpc, 100)) %>%  # Adjust 'breaks' to change number of bins
  group_by(dpc_pctile) %>%
  summarize(mean_rep = mean(pct_vote_rep, na.rm = TRUE),
            mean_demo = mean(pct_vote_demo, na.rm = TRUE)) %>% 
  mutate(bin = 1:n())

binned_data2 <- data %>%
  mutate(dpc_pctile = ntile(mean_household_income, 100)) %>%  # Adjust 'breaks' to change number of bins
  group_by(dpc_pctile) %>%
  summarize(mean_rep = mean(pct_vote_rep, na.rm = TRUE),
            mean_demo = mean(pct_vote_demo, na.rm = TRUE)) %>% 
  mutate(bin = 1:n(),
         sum = mean_rep + mean_demo)

# Plotting
ggplot(binned_data, aes(x = bin, y = mean_rep)) +
  geom_point() +  # or geom_line() depending on your preference
  geom_smooth(method = "lm",formula = y ~ poly(x, 4), se=FALSE) +
  theme_minimal() +
  labs(x = "Binned mortality per case ", y = "Mean vote for Trump",
       title = "Votes for Trump vs. MPC")
ggplot(binned_data, aes(x = bin, y = mean_demo)) +
  geom_point() +  # or geom_line() depending on your preference
  geom_smooth(method = "lm", formula = y ~ poly(x, 4),se=FALSE) +
  theme_minimal() +
  labs(x = "Binned mortality per case ", y = "Mean vote for Biden", 
       title = "Votes for Biden vs. MPC")

ggplot(binned_data2, aes(x = bin, y = mean_rep)) +
  geom_point() +  # or geom_line() depending on your preference
  geom_smooth(method = "lm", formula = y ~ poly(x, 2), se=FALSE) +
  theme_minimal() +
  labs(x = "Binned income levels", y = "Mean vote for Trump", 
       title = "Votes for Trump vs. Income")
ggplot(binned_data2, aes(x = bin, y = mean_demo)) +
  geom_point() +  # or geom_line() depending on your preference
  geom_smooth(method = "lm", formula = y ~ poly(x, 2), se=FALSE) +
  theme_minimal() +
  labs(x = "Binned income levels", y = "Mean vote for Biden",
       title = "Votes for Biden vs. Income")
```


```{r}
#| echo: false
#| message: false
#| fig.width: 10 
#| fig.height: 10
#| label: fig-map
#| fig-cap: The ratio of votes for the Republican and the infection rate per 100k in each county

p1 <- ggplot(data = merged_data, aes(x = long, y = lat, group = group)) + 
  geom_polygon(aes(fill = pct_vote_rep), color = "black") +
  scale_fill_distiller(palette = "RdBu", direction = -1, na.value = "grey") +
  labs(title = "The relative share of votes between Republican and Democrat party",
       fill = 'Share of Republican') +
  coord_quickmap() +
  theme_void() +
  geom_path(data = state, aes(x = long, y = lat, group = group), color = "black", linewidth = 0.5)

# Second plot
p2 <- ggplot(data = merged_data, aes(x = long, y = lat, group = group)) + 
  geom_polygon(aes(fill = dpc), color = "black") +
  scale_fill_gradient(low = "white", high = "darkblue", na.value = "grey",
                      limits = c(0, 5000), oob = scales::squish) +
  labs(title = "The distribution of death per case rate by 100K",
       fill = 'Death per cases by 100k') +
  coord_quickmap() +
  theme_void() +
  geom_path(data = state, aes(x = long, y = lat, group = group), color = "black", linewidth = 0.5)

p3 <- ggplot(data = merged_data, aes(x = long, y = lat, group = group)) + 
  geom_polygon(aes(fill = mean_household_income), color = "black") +
  scale_fill_gradient(low = "white", high = "darkblue", na.value = "grey",
                      limits = c(60000, 120000), oob = scales::squish) +
  labs(title = "The distribution of mean household income",
       fill = 'Mean Hosehold Income') +
  coord_quickmap() +
  theme_void() +
  geom_path(data = state, aes(x = long, y = lat, group = group), color = "black", linewidth = 0.5)

# Arrange the plots in a 2x1 layout
grid.arrange(p1, p3, p2, ncol = 1)
```

```{r}
#| eval: false
library(sp)
library(geojsonio)
library(ggplot2)
library(dplyr)
library(broom)
hex_county <- geojson_read("us_county_hexgrid.geojson", what = "sp")

# Reformat the data as needed (depending on your data structure)
hex_county@data <- hex_county@data %>% 
  mutate(adjusted_field = gsub("some_pattern", "replacement", original_field))

# Fortify the data for ggplot
hex_county_fortified <- tidy(hex_county, region = "desired_region_field")

# Plot the hexagon map
ggplot() +
  geom_polygon(data = hex_county_fortified, aes(x = long, y = lat, group = group), fill="#a1dab4", color="#f7f7f7") +
  geom_text(data = hex_county_fortified, aes(x = long, y = lat, label = label_field), size = 3) +
  theme_void() +
  coord_map()
```

## Data Limitation

Although the merged data covers the voting patterns, socio-economic characteristics and COVID-19 infection rate by county, some counties are missing. From the above two maps, the counties in the middle west show grey, indicating that the data for these counties are absent. This absence of data may increase bias and influence the accuracy of data analyses.

# Methods

This paper aims to make a causal inference between the 2020 voting patterns and COVID-19 to find whether Trump can contribute to his loss of COVID-19. To see the causal effect in the observational study, I will implement the methods of propensity scores to find the change in votes for Trump due to the pandemic. Based on the results, I will also conduct a counterfactual analysis to see what would have happened if there had been no COVID-19,

First and foremost, we need to define the treatment. The treatment in an experiment will be artificially imposed on specific groups of people to see the effect of interested variables with and without this treatment. Therefore, the people with the treatment are called the treatment group, and the rest are called the control group. The treatment and control group settings are to test whether the imposed factor impacts the interested variable. However, since this is an observational study, it is unrealistic to set a factor manually. Therefore, to find the causal effect of COVID-19 on Trump's voting, I will choose the treatment to be whether a county has a high infection rate.

## Propensity Score

Propensity score is a technique that can help us to decide which county would be in the treatment group. That said, the propensity score in this study is the probability of a county having a high infection rate, given a set of socioeconomic variables. Using the propensity score can ensure that the two counties with similar propensity scores have the same distribution of the observed socioeconomic variables, such as income level, even though they are in different groups. This is fantastic, as we can have many pairs from the treatment of the control group with similar covariates by fixing propensity scores. The key drawback is that we can not estimate the causal effect from observational studies.

In addition, by finding the propensity scores, we can reconstruct our assignment mechanism to a strongly ignorable one. An assignment mechanism is "strongly ignorable" if and only if, given the observed covariates, the potential outcomes are independent of the treatment assignment. This indicates that the voting change for Trump is independent of our assignment that a county has a high or low infection rate.

As the propensity score represents the probability of a county having a high infection rate, we must find a logistic regression to predict this probability. In my previous paper, I already found the best model predicting the mortality rate of COVID in each US county. Even though that paper was to predict the mortality rate, the mortality is correlated with the infection rate. Therefore, I will take the dependent variables in that model to be the covariates in predicting the propensity score for each county, which is:

```{=tex}
\begin{align*}
\frac{PS}{1-PS} &=  \beta_0 + \\
& + \beta_1 \times \text{prop\_higher\_education} \\
& + \beta_2 \times \text{IncomePctile} \\
& + \beta_3 \times \text{no\_insurance} \\
& + \beta_4 \times \text{private\_insurance} \\
& + \beta_5 \times \text{males} \\
& + \beta_6 \times \text{old\_85} \\
& + \beta_7 \times \text{white\_pct} \\
& + \beta_8 \times \text{black\_pct}
\end{align*}
```

Therefore, we can find the PS for each county using the above model. However, there are three ways to use the propensity scores to find the treatment effect.

### Propensity Score Matching
The package needed to implement the matching is the `Matching.` Using this package, we can match the observations in the treatment and control groups having the same or similar propensity scores. Then, take only the matched pairs to be the new data set and find the treatment effect between them. In addition, we can also check the balance between the treatment and control groups. 

However, the most significant drawback is that the new data set can only contain the matched pairs. We may lose a considerable amount of observations during this process. Our predictions may also not be accurate.

### Propensity Score Stratification
As introduced above, using PSM may cause the waste of data and results in bias. However, propensity score stratification will use the entire data set to avoid waste. The way to do this is to split the data into different quantiles and fit a model for each subclass, with the only dependent variable being the treatment variable. Then, calculate the average casual effect for the subclasses.

Even though the PSS can use the entire data set, however, the choice of quantiles will be closely related to the predictions. If we have a large sample and want to split the data into many subclasses, then this method may be computationally slow.

### Propensity Score Regression Adjustment
The last method is the most straightforward, meaning we just fit a regression model with the treatment variable and propensity score as the two dependent variables.

Each of the above three methods has its unique advantages, but the results may vary between different data sets. I will perform both of the three methods and compare which one would be the best.


## Counterfactual Analysis
Using the propensity score methodology, we can assess the causal effect of a high infection rate on the voting for Trump. However, it can not tell us the election results, and we can not draw conclusions about COVID and the election. Therefore, I will conduct a counterfactual analysis, meaning that I will re-calculate the votes for Trump and Biden to see if the final election results will change. 

Eventually, I will re-calculate the votes for the two candidates to follow the official election rule. By finding the difference in voting percentage, I will re-allocate the voting for the two parties and calculate the electoral votes in each state using the winner-takes-all rule.

# Results

```{r}
#| echo: false
#| warning: false
#| message: false
data <- read.csv(here("outputs/data/merged_data.csv"))

highinf_model <- glm(treatment ~ prop_higher_education + pctile + 
                       no_insurance + private_insurance + males +
                       white_pct + black_pct, family = binomial(),
                      data = data)
# summary(highinf_model)
data$prop_score <- predict(highinf_model, type = "response")
```

```{r}
data <- read.csv(here("outputs/data/merged_data.csv"))

library(MatchIt)
m.out <- matchit(treatment ~ prop_higher_education + pctile + 
                       no_insurance + private_insurance + males +
                       white_pct + black_pct,
                 data = data,
                 distance = "logit",
                 method = "nearest",
                 replace = FALSE,
                 ratio = 1)
summary(m.out, standardize = TRUE)
```

```{=tex}
\begin{align*}
\text{Logit}(\text{PS}) &= -2.008 + \\
& -0.032 \times \text{prop\_higher\_education} \\
& -0.009 \times \text{IncomePctile} \\
& +0.088 \times \text{no\_insurance} \\
& +0.051 \times \text{private\_insurance} \\
& +0.019 \times \text{males} \\
& +0.055 \times \text{old\_85} \\
& -0.030 \times \text{white\_pct} \\
& +0.026 \times \text{black\_pct}
\end{align*}
```

The above equation shows the logistic regression model to predict the propensity score for each county, given the covariates. It seems that insurance is a critical factor in the infection rate. Interestingly, the counties with a higher proportion of people with no insurance and private insurance will both increase the probability of this county having a high infection rate. Using this model, we can find the propensity score for each county and implement the three methods.
```{r}
model <- lm(pct_vote_rep ~ dpc + mean_household_income, data = data)
summary(model)
```

```{r}
#| echo: false
#| message: false

library(Matching)
# Propensity score matching
rr <- Match(Y = data$pct_vote_rep, Tr = data$treatment, 
            X = data$prop_score, M = 1)
summary(rr)
```

```{r}

```

```{r}
#| echo: false
#| message: false
#| label: tbl-PSS
#| tbl-cap: The results of causal effect using propensity score statification
#| eval: false
# PSS
sub <- quantile(data$prop_score, probs = c(0.2, 0.4, 0.6, 0.8))

sub1 <- glm(pct_vote_rep ~ treatment,
            data = data[data$prop_score <= sub[1], ])
sub2 <- glm(pct_vote_rep ~ treatment,
            data = data[data$prop_score > sub[1] & data$prop_score <= sub[2], ])

sub3 <- glm(pct_vote_rep ~ treatment,
            data = data[data$prop_score > sub[2] & data$prop_score <= sub[3], ])

sub4 <- glm(pct_vote_rep ~ treatment,
            data = data[data$prop_score > sub[3] & data$prop_score <= sub[4], ])

sub5 <- glm(pct_vote_rep ~ treatment,
            data = data[data$prop_score > sub[4], ])

# Extracting the necessary information from each model
extract_info <- function(model, quantile) {
  coef_summary <- summary(model)$coefficients
  data <- data.frame(
    quantile = quantile,
    ce = coef_summary["treatment", "Estimate"],
    p_val = coef_summary["treatment", "Pr(>|t|)"]
  )
  return(data)
}

# Creating data frames for each model
df1 <- extract_info(sub1, "0-20%")
df2 <- extract_info(sub2, "20-40%")
df3 <- extract_info(sub3, "40-60%")
df4 <- extract_info(sub4, "60-80%")
df5 <- extract_info(sub5, "80-100%")

# Combining all data frames into one
rbind(df1, df2, df3, df4, df5) %>%
  rename(`PS Quantile` = quantile,
         `Estimated CE` = ce,
         `P Value` = p_val) %>%
  kable()
```

```{r}
#| echo: false
#| message: false
# Propensity score regression adjustment
model_adj <- glm(change_vote_rep ~ treatment + prop_score, data = data)
summary(model_adj)
```

```{r}
#| echo: false
#| message: false
#| eval: false
# Counterfactual Analysis
adjusted_data <- data %>% 
  dplyr::select(state, county, fips, party, votes, total_votes, pct_vote, high_infrate)

adjust_function <- function(data){
  adjust_votes <- c()
  for (i in 1:nrow(data)){
    if (data[i,"high_infrate"] == 1){
      if (data[i,"party"] == "Republican"){
        adjust_votes[i] <- round((data[i,"pct_vote"] + 0.036928 ) * data[i,"total_votes"], 0)
      }
      else if (data[i,"party"] == "Democrat"){
        adjust_votes[i] <- round((data[i,"pct_vote"] - 0.036928 ) * data[i,"total_votes"], 0)
      }
      else {adjust_votes[i] <- data[i,"votes"]}
    }
    else {
      adjust_votes[i] <- data[i,"votes"]
    }
  }
  return(adjust_votes)
}



adjusted_data %>% 
  group_by(state, party) %>% 
  summarise(votes = sum(votes), .groups = "drop")
```

```{r}
#| echo: false
#| message: false
#| eval: false
adjust_function2 <- function(data) {
  adjust_votes <- numeric(nrow(data))  # Initialize the vector with the correct length
  extra_vote_percentage <- 0.036928
  
  for (i in 1:nrow(data)) {
    if (data[i, "high_infrate"] == 1) {
      total_votes <- data[i, "total_votes"]
      extra_votes <- round(extra_vote_percentage * total_votes, 0)
      
      if (data[i, "party"] == "Republican") {
        adjust_votes[i] <- data[i, "votes"] + extra_votes
      } else {
        # Calculate total votes for non-Republican parties
        total_non_rep_votes <- sum(data[data$party != "Republican" & data$fips == data[i, "fips"], "votes"])
        
        # Distribute the vote loss proportionally
        if (total_non_rep_votes > 0) {
          party_vote_share <- data[i, "votes"] / total_non_rep_votes
          votes_lost <- round(extra_votes * party_vote_share, 0)
          adjust_votes[i] <- max(0, data[i, "votes"] - votes_lost)
        } else {
          adjust_votes[i] <- data[i, "votes"]
        }
      }
    } else {
      adjust_votes[i] <- data[i, "votes"]
    }
  }
  
  return(adjust_votes)
}

adjust_vote1 <- adjust_function(adjusted_data)
adjust_vote2 <- adjust_function2(adjusted_data)
adjusted_data$adjust_vote1 <- adjust_vote1
adjusted_data$adjust_vote2 <- adjust_vote2
```

# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this.

## Second discussion point

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

# References
